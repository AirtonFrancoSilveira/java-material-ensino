# üìä Guia de Estudo: Apache Beam e Google Cloud Dataflow

**Laborat√≥rio:** Serverless Data Processing with Dataflow - Writing an ETL pipeline using Apache Beam and Dataflow (Java)  
**Link:** https://www.cloudskillsboost.google/focuses/64779

---

## üéØ Conceitos Fundamentais

### Apache Beam
- **Modelo unificado** para processamento de dados em batch e streaming
- **Port√°vel**: pode executar em diferentes runners (Dataflow, Flink, Spark)
- **Linguagens**: Java, Python, Go
- **Open Source** com amplo suporte da comunidade

### Google Cloud Dataflow
- **Servi√ßo gerenciado** para executar pipelines Apache Beam
- **Escalabilidade autom√°tica** de recursos computacionais
- **Serverless**: sem necessidade de gerenciar infraestrutura
- **Integra√ß√£o nativa** com outros servi√ßos GCP

---

## üèóÔ∏è Arquitetura do Pipeline

### Fluxo ETL Implementado
```
[Google Cloud Storage] ‚Üí [Apache Beam Pipeline] ‚Üí [BigQuery]
     events.json       ‚Üí   Read ‚Üí Transform ‚Üí Write  ‚Üí   logs.logs
```

### Transforma√ß√µes
```java
// 1. Leitura
PCollection<String> events = pipeline.apply("ReadLines", TextIO.read().from(input));

// 2. Transforma√ß√£o
PCollection<CommonLog> commonLogs = events.apply("JsonToCommonLog", ParDo.of(new JsonToCommonLog()));

// 3. Escrita
commonLogs.apply("WriteToBigQuery", BigQueryIO.<CommonLog>write().to(output).useBeamSchema());
```

---

## üíª C√≥digo Completo Desenvolvido

### Classe Principal (MyPipeline.java)
```java
package com.mypackage.pipeline;

import com.google.gson.Gson;
import org.apache.beam.runners.dataflow.options.DataflowPipelineOptions;
import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.PipelineResult;
import org.apache.beam.sdk.io.TextIO;
import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;
import org.apache.beam.sdk.options.PipelineOptionsFactory;
import org.apache.beam.sdk.schemas.JavaFieldSchema;
import org.apache.beam.sdk.schemas.annotations.DefaultSchema;
import org.apache.beam.sdk.transforms.DoFn;
import org.apache.beam.sdk.transforms.ParDo;
import org.apache.beam.sdk.values.PCollection;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class MyPipeline {
    private static final Logger LOG = LoggerFactory.getLogger(MyPipeline.class);

    public interface Options extends DataflowPipelineOptions {
    }

    public static void main(String[] args) {
        Options options = PipelineOptionsFactory.fromArgs(args).as(Options.class);
        run(options);
    }

    // Classe para representar os dados
    @DefaultSchema(JavaFieldSchema.class)
    public static class CommonLog {
        public String timestamp;
        public String user_id;
        public String http_request;
        public Long http_response;
        public String user_agent;
        public Double lat;
        public Double lng;
        public String ip;
        public Long num_bytes;
        
        public CommonLog() {}
    }

    // Transforma√ß√£o para converter JSON em CommonLog
    public static class JsonToCommonLog extends DoFn<String, CommonLog> {
        @ProcessElement
        public void processElement(@Element String json, OutputReceiver<CommonLog> receiver) {
            try {
                Gson gson = new Gson();
                CommonLog commonLog = gson.fromJson(json, CommonLog.class);
                receiver.output(commonLog);
            } catch (Exception e) {
                LOG.error("Erro ao processar JSON: " + json, e);
            }
        }
    }

    // M√©todo principal do pipeline
    public static PipelineResult run(Options options) {
        Pipeline pipeline = Pipeline.create(options);
        options.setJobName("my-pipeline-" + System.currentTimeMillis());

        final String input = "gs://qwiklabs-gcp-00-be3eebd5bbbf/events.json";
        final String output = "qwiklabs-gcp-00-be3eebd5bbbf:logs.logs";

        // Pipeline ETL
        PCollection<String> events = pipeline
            .apply("ReadLines", TextIO.read().from(input));

        PCollection<CommonLog> commonLogs = events
            .apply("JsonToCommonLog", ParDo.of(new JsonToCommonLog()));

        commonLogs.apply("WriteToBigQuery", 
            BigQueryIO.<CommonLog>write()
                .to(output)
                .useBeamSchema()
                .withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_TRUNCATE)
        );

        LOG.info("Building pipeline...");
        return pipeline.run();
    }
}
```

### Depend√™ncias Maven (pom.xml)
```xml
<dependencies>
    <!-- Apache Beam Core -->
    <dependency>
        <groupId>org.apache.beam</groupId>
        <artifactId>beam-sdks-java-core</artifactId>
        <version>2.63.0</version>
    </dependency>
    
    <!-- Runners -->
    <dependency>
        <groupId>org.apache.beam</groupId>
        <artifactId>beam-runners-direct-java</artifactId>
        <version>2.63.0</version>
    </dependency>
    <dependency>
        <groupId>org.apache.beam</groupId>
        <artifactId>beam-runners-google-cloud-dataflow-java</artifactId>
        <version>2.63.0</version>
    </dependency>
    
    <!-- GCP Integration -->
    <dependency>
        <groupId>org.apache.beam</groupId>
        <artifactId>beam-sdks-java-io-google-cloud-platform</artifactId>
        <version>2.63.0</version>
    </dependency>
    
    <!-- Extensions -->
    <dependency>
        <groupId>org.apache.beam</groupId>
        <artifactId>beam-sdks-java-extensions-google-cloud-platform-core</artifactId>
        <version>2.63.0</version>
    </dependency>
    
    <!-- Logging -->
    <dependency>
        <groupId>org.slf4j</groupId>
        <artifactId>slf4j-api</artifactId>
        <version>1.7.25</version>
    </dependency>
    <dependency>
        <groupId>org.slf4j</groupId>
        <artifactId>slf4j-simple</artifactId>
        <version>1.7.25</version>
    </dependency>
</dependencies>
```

---

## üö® Problemas Encontrados e Solu√ß√µes

### 1. Erro de Mapeamento JSON
**Problema:**
```
RuntimeException: Null value set on non-nullable field Field{name=event}
```

**Causa:** Schema da classe `CommonLog` n√£o correspondia aos dados reais.

**Solu√ß√£o:** Adaptar classe aos dados reais:
```java
// Dados reais tinham campos diferentes
public String http_request;    // em vez de "event"
public Long http_response;     // em vez de "session_id"  
public String user_agent;     // campo adicional
public Double lat, lng;       // coordenadas
public String ip;             // IP do usu√°rio
public Long num_bytes;        // bytes transferidos
```

### 2. Erro de Localiza√ß√£o no Dataflow
**Problema:**
```
'us-central1' violates constraint 'constraints/gcp.resourceLocations'
```

**Solu√ß√µes:**
- Tentar regi√£o diferente: `us-east1`
- Voltar para DirectRunner para desenvolvimento local
- Verificar regi√µes: `gcloud dataflow regions list`

### 3. Erro de TempLocation
**Problema:**
```
BigQueryIO.Write needs a GCS temp location to store temp files
```

**Solu√ß√£o:**
```bash
--tempLocation=gs://bucket-name/temp
```

---

## ‚ö° Comandos Importantes

### Configura√ß√£o do Ambiente
```bash
export PROJECT_ID=$(gcloud config get-value project)
export REGION='us-central1'
export PIPELINE_FOLDER=gs://${PROJECT_ID}
export MAIN_CLASS_NAME=com.mypackage.pipeline.MyPipeline
export RUNNER=DataflowRunner
```

### Execu√ß√£o Local (DirectRunner)
```bash
export RUNNER=DirectRunner
mvn compile exec:java \
-Dexec.mainClass=${MAIN_CLASS_NAME} \
-Dexec.args="--tempLocation=gs://bucket-name/temp"
```

### Execu√ß√£o na Nuvem (DataflowRunner)
```bash
export RUNNER=DataflowRunner
mvn compile exec:java \
-Dexec.mainClass=${MAIN_CLASS_NAME} \
-Dexec.cleanupDaemonThreads=false \
-Dexec.args=" \
--project=${PROJECT_ID} \
--region=${REGION} \
--stagingLocation=${PIPELINE_FOLDER}/staging \
--tempLocation=${PIPELINE_FOLDER}/temp \
--runner=${RUNNER}"
```

### BigQuery Operations
```bash
# Listar datasets
bq ls

# Listar tabelas
bq ls logs

# Consultar dados
bq query --use_legacy_sql=false \
'SELECT COUNT(*) as total FROM logs.logs'

# Ver schema
bq show --schema logs.logs

# Amostra de dados
bq query --use_legacy_sql=false \
'SELECT * FROM logs.logs LIMIT 10'
```

---

## üìä Estruturas de Dados

### Exemplo de Dados de Entrada (JSON)
```json
{
  "timestamp": "2025-06-18T19:43:26.971204Z",
  "user_id": "7739795499273709612",
  "http_request": "\"GET coniferophyta.html HTTP/1.0\"",
  "http_response": 200,
  "user_agent": "Mozilla/5.0 (Windows NT 4.0) AppleWebKit/536.1",
  "lat": 39.969,
  "lng": -83.0114,
  "ip": "21.68.16.223",
  "num_bytes": 387
}
```

### Schema BigQuery Resultante
| Campo         | Tipo    | Modo     |
|---------------|---------|----------|
| timestamp     | STRING  | REQUIRED |
| user_id       | STRING  | REQUIRED |
| http_request  | STRING  | REQUIRED |
| http_response | INTEGER | REQUIRED |
| user_agent    | STRING  | REQUIRED |
| lat           | FLOAT   | REQUIRED |
| lng           | FLOAT   | REQUIRED |
| ip            | STRING  | REQUIRED |
| num_bytes     | INTEGER | REQUIRED |

---

## üèÉ‚Äç‚ôÇÔ∏è Runners: DirectRunner vs DataflowRunner

### DirectRunner
**Caracter√≠sticas:**
- ‚úÖ Execu√ß√£o local
- ‚úÖ Debugging r√°pido
- ‚úÖ Ideal para desenvolvimento
- ‚ùå N√£o escal√°vel
- ‚ùå Limitado pelos recursos da m√°quina

### DataflowRunner  
**Caracter√≠sticas:**
- ‚úÖ Execu√ß√£o distribu√≠da na nuvem
- ‚úÖ Escalabilidade autom√°tica
- ‚úÖ Monitoramento em tempo real
- ‚ùå Mais lento para iniciar
- ‚ùå Custos de computa√ß√£o

### Quando Usar Cada Um
| Cen√°rio              | Runner Recomendado |
|----------------------|-------------------|
| Desenvolvimento      | DirectRunner      |
| Testes              | DirectRunner      |
| Debugging           | DirectRunner      |
| Produ√ß√£o            | DataflowRunner    |
| Datasets grandes    | DataflowRunner    |
| Processamento cr√≠tico| DataflowRunner   |

---

## üîå BigQuery Integration

### Write Dispositions
```java
// Recriar tabela (desenvolvimento)
.withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_TRUNCATE)

// Adicionar dados (produ√ß√£o)
.withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_APPEND)

// Apenas se vazia (seguran√ßa)
.withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_EMPTY)
```

### Schema Autom√°tico vs Manual
```java
// Autom√°tico com @DefaultSchema
.useBeamSchema()

// Manual
TableSchema schema = new TableSchema().setFields(Arrays.asList(
    new TableFieldSchema().setName("timestamp").setType("STRING").setMode("REQUIRED")
));
.withSchema(schema)
```

---

## üéØ Melhores Pr√°ticas

### 1. Estrutura do C√≥digo
- Separar transforma√ß√µes em classes pr√≥prias
- Usar nomes descritivos para transforma√ß√µes
- Implementar tratamento de erros robusto
- Logging adequado para debugging

### 2. Gest√£o de Schema
- Validar dados de entrada antes do processamento
- Usar tipos apropriados (Long para IDs, Double para coordenadas)
- Considerar campos opcionais quando apropriado
- Documentar mudan√ßas de schema

### 3. Performance
- Evitar opera√ß√µes custosas dentro de DoFn
- Usar PipelineOptions para configura√ß√µes
- Considerar paraleliza√ß√£o adequada
- Monitorar m√©tricas de execu√ß√£o

### 4. Desenvolvimento
- Testar localmente com DirectRunner primeiro
- Usar datasets pequenos para desenvolvimento
- Implementar testes unit√°rios para DoFn
- Versionamento de c√≥digo adequado

---

## üöÄ Pr√≥ximos Passos de Estudo

### 1. Conceitos Avan√ßados
- **Windowing** para dados streaming
- **Triggers** para processamento temporal
- **Side Inputs** para dados auxiliares
- **State e Timers** para casos complexos

### 2. Padr√µes de Pipeline
- **Branching**: dividir pipeline em m√∫ltiplos caminhos
- **Joining**: combinar diferentes PCollections
- **Aggregations**: opera√ß√µes de soma, m√©dia, contagem
- **Error Handling**: dead letter queues

### 3. Integra√ß√µes
- **Apache Kafka** para streaming
- **Cloud Pub/Sub** para mensagens
- **Cloud Spanner** para dados relacionais
- **Cloud ML** para machine learning

### 4. Templates e Parametriza√ß√£o
- **Dataflow Templates** para reutiliza√ß√£o
- **Flex Templates** para maior flexibilidade
- **Pipeline Options** personalizadas
- **Configura√ß√£o externa** via arquivos

---

## üìö Recursos Adicionais

### Documenta√ß√£o Oficial
- [Apache Beam Programming Guide](https://beam.apache.org/documentation/programming-guide/)
- [Google Cloud Dataflow Documentation](https://cloud.google.com/dataflow/docs)
- [BigQuery Documentation](https://cloud.google.com/bigquery/docs)

### Exemplos Pr√°ticos
- [Apache Beam Examples](https://github.com/apache/beam/tree/master/examples/java)
- [Google Cloud Dataflow Templates](https://github.com/GoogleCloudPlatform/DataflowTemplates)

---

## üéì Resumo do Aprendizado

### O que Desenvolvemos
1. ‚úÖ **Pipeline ETL completo** de GCS para BigQuery
2. ‚úÖ **Transforma√ß√£o JSON** com Gson
3. ‚úÖ **Schema autom√°tico** com Beam annotations
4. ‚úÖ **Execu√ß√£o local e na nuvem**
5. ‚úÖ **Tratamento de erros** e debugging

### Compet√™ncias Adquiridas
- **Apache Beam**: conceitos fundamentais e desenvolvimento
- **Google Cloud Dataflow**: execu√ß√£o e monitoramento
- **BigQuery**: integra√ß√£o e schema management
- **Java**: programa√ß√£o com frameworks de big data
- **DevOps**: deployment e troubleshooting

### Aplica√ß√µes Pr√°ticas
- **ETL pipelines** para data warehouses
- **Data lake processing** para analytics
- **Real-time streaming** para dashboards
- **ML data preparation** para modelos
- **Log processing** para observabilidade

---

**üìù Documento criado:** Junho 2025  
**üîó Laborat√≥rio base:** [Cloud Skills Boost - Apache Beam ETL](https://www.cloudskillsboost.google/focuses/64779)  
**üí° Objetivo:** Refer√™ncia de estudo para Apache Beam e Dataflow 